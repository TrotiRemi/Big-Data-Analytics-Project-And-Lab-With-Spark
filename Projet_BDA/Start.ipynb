{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "431770d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.1\n",
      "PySpark version: 4.0.1\n",
      "Python version: 3.10.19\n",
      "Session timezone: UTC\n",
      "Shuffle partitions: 4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import platform\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName('BDA-Project')\n",
    "    .config('spark.sql.session.timeZone', 'UTC')\n",
    "    .config('spark.sql.shuffle.partitions', '4')\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel('WARN')\n",
    "\n",
    "print(f'Spark version: {spark.version}')\n",
    "print(f'PySpark version: {pyspark.__version__}')\n",
    "print(f'Python version: {sys.version.split()[0]}')\n",
    "print(f'Session timezone: {spark.conf.get(\"spark.sql.session.timeZone\")}')\n",
    "print(f'Shuffle partitions: {spark.conf.get(\"spark.sql.shuffle.partitions\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c05d1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PHASE 1: BLOCKCHAIN + PRICES INTEGRATION - FULL SPARK\n",
      "======================================================================\n",
      "\n",
      "ðŸ“– Etape 1: Chargement des donnees de prix...\n",
      "âœ“ 68543 heures chargees\n",
      "\n",
      "ðŸ”— Etape 2: Preparation des donnees blockchain...\n",
      "âœ… DONNEES REELLES TROUVEES - Utilisation des vraies donnees blockchain!\n",
      "   âœ“ 1888 heures de donnees reelles chargees\n",
      "   âœ“ Periode: 2012-06-09 22:00:00+00:00 a 2012-08-30 09:00:00+00:00\n",
      "\n",
      "ðŸ’¾ Etape 3: Standardisation et sauvegarde en Parquet...\n",
      "âœ“ Blockchain sauvegardee: data/parquet/blockchain_hourly.parquet\n",
      "   Type: REELLES\n",
      "\n",
      "âš¡ Etape 4: Chargement des donnees dans Spark...\n",
      "âœ“ Prices charges dans Spark: 68543 rows\n",
      "\n",
      "ðŸ“Š Etape 5: Chargement blockchain dans Spark...\n",
      "âœ“ Blockchain charge dans Spark: 1888 rows\n",
      "   Source: DONNEES REELLES\n",
      "\n",
      "ðŸ”„ Etape 6: JOIN blockchain + prices en Spark SQL...\n",
      "âœ“ JOIN complete: 68543 rows\n",
      "\n",
      "ðŸ’¾ Etape 7: Cache du DataFrame integre...\n",
      "âœ“ Spark DataFrame final cree et cache!\n",
      "  Nombre de lignes: 68543\n",
      "  Nombre de colonnes: 19\n",
      "\n",
      "âœ… PHASE 1 COMPLETEE - FULL SPARK!\n",
      "   df_integrated: DataFrame Spark avec blockchain + prices\n",
      "   UTILISE DONNEES REELLES extraites des fichiers .dat\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 1: BLOCKCHAIN + PRICES INTEGRATION - FULL SPARK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nðŸ“– Etape 1: Chargement des donnees de prix...\")\n",
    "df_prices_pd = pd.read_csv(\"data/prices/btc_1h_data_2018_to_2025.csv\")\n",
    "print(f\"âœ“ {len(df_prices_pd)} heures chargees\")\n",
    "\n",
    "print(\"\\nðŸ”— Etape 2: Preparation des donnees blockchain...\")\n",
    "\n",
    "blockchain_real_parquet = \"data/parquet/blockchain_real_hourly.parquet\"\n",
    "\n",
    "if os.path.exists(blockchain_real_parquet):\n",
    "    print(\"âœ… DONNEES REELLES TROUVEES - Utilisation des vraies donnees blockchain!\")\n",
    "    df_hourly_blockchain = pd.read_parquet(blockchain_real_parquet)\n",
    "    df_hourly_blockchain['timestamp'] = pd.to_datetime(df_hourly_blockchain['timestamp'])\n",
    "    print(f\"   âœ“ {len(df_hourly_blockchain)} heures de donnees reelles chargees\")\n",
    "    print(f\"   âœ“ Periode: {df_hourly_blockchain['timestamp'].min()} a {df_hourly_blockchain['timestamp'].max()}\")\n",
    "    is_real_blockchain = True\n",
    "else:\n",
    "    print(\"âš ï¸  Donnees blockchain reelles non trouvees - Generation synthetiques de fallback...\")\n",
    "    \n",
    "    transactions = []\n",
    "\n",
    "    for idx, row in df_prices_pd.iterrows():\n",
    "        try:\n",
    "            timestamp_str = row['Open time']\n",
    "            close_price = float(row['Close'])\n",
    "            volume = float(row['Volume']) if pd.notna(row['Volume']) else 0\n",
    "            \n",
    "            if pd.api.types.is_numeric_dtype(timestamp_str):\n",
    "                timestamp = int(timestamp_str)\n",
    "            else:\n",
    "                try:\n",
    "                    dt = datetime.fromisoformat(str(timestamp_str).replace('Z', '+00:00'))\n",
    "                    timestamp = int(dt.timestamp())\n",
    "                except:\n",
    "                    dt = pd.to_datetime(timestamp_str)\n",
    "                    timestamp = int(dt.timestamp())\n",
    "            \n",
    "            base_tx_count = int(100 + (volume / 100000)) if volume > 0 else 100\n",
    "            tx_count = max(50, min(500, base_tx_count + random.randint(-50, 50)))\n",
    "            \n",
    "            for tx_idx in range(tx_count):\n",
    "                tx_timestamp = timestamp + random.randint(0, 3599)\n",
    "                avg_amount_btc = max(0.01, 10 / (close_price / 1000)) if close_price > 0 else 0.5\n",
    "                amount_btc = avg_amount_btc * random.uniform(0.5, 2.0)\n",
    "                amount_satoshi = int(amount_btc * 100_000_000)\n",
    "                \n",
    "                tx = {\n",
    "                    'tx_hash': f\"tx_{idx}_{tx_idx}\",\n",
    "                    'timestamp': tx_timestamp,\n",
    "                    'input_count': random.randint(1, 5),\n",
    "                    'output_count': random.randint(1, 5),\n",
    "                    'total_output_value': amount_satoshi,\n",
    "                    'block_hash': f\"block_{idx}\"\n",
    "                }\n",
    "                transactions.append(tx)\n",
    "            \n",
    "            if (idx + 1) % 500 == 0:\n",
    "                print(f\"  âœ“ {idx + 1}/{len(df_prices_pd)} heures...\")\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    print(f\"âœ“ {len(transactions)} transactions synthetiques generees\")\n",
    "\n",
    "    df_txs = pd.DataFrame(transactions)\n",
    "    df_txs['hour_dt'] = pd.to_datetime(df_txs['timestamp'], unit='s').dt.floor('h')\n",
    "\n",
    "    df_hourly_blockchain = df_txs.groupby('hour_dt').agg({\n",
    "        'tx_hash': 'count',\n",
    "        'input_count': 'mean',\n",
    "        'output_count': 'mean',\n",
    "        'total_output_value': ['sum', 'mean', 'std']\n",
    "    }).reset_index()\n",
    "\n",
    "    df_hourly_blockchain.columns = [\n",
    "        'timestamp', 'tx_count', 'avg_inputs', 'avg_outputs',\n",
    "        'total_volume_satoshi', 'avg_amount_satoshi', 'std_amount_satoshi'\n",
    "    ]\n",
    "\n",
    "    for col_name in ['total_volume_satoshi', 'avg_amount_satoshi', 'std_amount_satoshi']:\n",
    "        df_hourly_blockchain[col_name] = df_hourly_blockchain[col_name] / 100_000_000\n",
    "\n",
    "    df_hourly_blockchain = df_hourly_blockchain.rename(columns={\n",
    "        'total_volume_satoshi': 'total_volume_btc',\n",
    "        'avg_amount_satoshi': 'avg_amount_btc',\n",
    "        'std_amount_satoshi': 'std_amount_btc'\n",
    "    })\n",
    "\n",
    "    print(f\"âœ“ {len(df_hourly_blockchain)} heures aggregees\")\n",
    "    is_real_blockchain = False\n",
    "\n",
    "print(\"\\nðŸ’¾ Etape 3: Standardisation et sauvegarde en Parquet...\")\n",
    "\n",
    "df_hourly_blockchain['timestamp'] = df_hourly_blockchain['timestamp'].astype(str)\n",
    "\n",
    "required_cols = ['timestamp', 'tx_count', 'avg_inputs', 'avg_outputs', 'total_volume_btc', 'avg_amount_btc', 'std_amount_btc']\n",
    "df_hourly_blockchain = df_hourly_blockchain[required_cols]\n",
    "\n",
    "os.makedirs(\"data/parquet\", exist_ok=True)\n",
    "blockchain_parquet = \"data/parquet/blockchain_hourly.parquet\"\n",
    "df_hourly_blockchain.to_parquet(blockchain_parquet)\n",
    "print(f\"âœ“ Blockchain sauvegardee: {blockchain_parquet}\")\n",
    "print(f\"   Type: {'REELLES' if is_real_blockchain else 'SYNTHETIQUES (fallback)'}\")\n",
    "\n",
    "print(\"\\nâš¡ Etape 4: Chargement des donnees dans Spark...\")\n",
    "df_prices_spark = spark.read.csv(\"data/prices/btc_1h_data_2018_to_2025.csv\", header=True, inferSchema=True)\n",
    "print(f\"âœ“ Prices charges dans Spark: {df_prices_spark.count()} rows\")\n",
    "\n",
    "print(\"\\nðŸ“Š Etape 5: Chargement blockchain dans Spark...\")\n",
    "df_blockchain_spark = spark.read.parquet(blockchain_parquet)\n",
    "print(f\"âœ“ Blockchain charge dans Spark: {df_blockchain_spark.count()} rows\")\n",
    "print(f\"   Source: {'DONNEES REELLES' if is_real_blockchain else 'donnees synthetiques'}\")\n",
    "\n",
    "print(\"\\nðŸ”„ Etape 6: JOIN blockchain + prices en Spark SQL...\")\n",
    "\n",
    "from pyspark.sql.functions import col, to_timestamp, date_trunc\n",
    "\n",
    "df_prices_spark = df_prices_spark.withColumn(\n",
    "    \"hour_ts\", \n",
    "    date_trunc('hour', to_timestamp(col(\"Open time\")))\n",
    ")\n",
    "\n",
    "df_blockchain_spark = df_blockchain_spark.withColumn(\n",
    "    \"hour_ts\", \n",
    "    to_timestamp(col(\"timestamp\"))\n",
    ")\n",
    "\n",
    "df_prices_spark.createOrReplaceTempView(\"prices\")\n",
    "df_blockchain_spark.createOrReplaceTempView(\"blockchain\")\n",
    "\n",
    "df_integrated = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        p.*,\n",
    "        b.tx_count,\n",
    "        b.avg_inputs,\n",
    "        b.avg_outputs,\n",
    "        b.total_volume_btc,\n",
    "        b.avg_amount_btc,\n",
    "        b.std_amount_btc\n",
    "    FROM prices p\n",
    "    LEFT JOIN blockchain b\n",
    "    ON p.hour_ts = b.hour_ts\n",
    "\"\"\")\n",
    "\n",
    "print(f\"âœ“ JOIN complete: {df_integrated.count()} rows\")\n",
    "\n",
    "print(\"\\nðŸ’¾ Etape 7: Cache du DataFrame integre...\")\n",
    "df_integrated.cache()\n",
    "\n",
    "print(f\"âœ“ Spark DataFrame final cree et cache!\")\n",
    "print(f\"  Nombre de lignes: {df_integrated.count()}\")\n",
    "print(f\"  Nombre de colonnes: {len(df_integrated.columns)}\")\n",
    "\n",
    "print(f\"\\nâœ… PHASE 1 COMPLETEE - FULL SPARK!\")\n",
    "print(f\"   df_integrated: DataFrame Spark avec blockchain + prices\")\n",
    "if is_real_blockchain:\n",
    "    print(f\"   UTILISE DONNEES REELLES extraites des fichiers .dat\")\n",
    "else:\n",
    "    print(f\"   (Utilise donnees synthetiques - pour ameliorer: executez BONUS PHASE)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43703cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PHASE 2: DATA CLEANING - Using Integrated Data\n",
      "======================================================================\n",
      "\n",
      "âœ“ Integrated dataset loaded: 68542 rows\n",
      "âœ“ Columns: 19\n",
      "\n",
      "âœ“ Data cleaned!\n",
      "âœ“ Final shape: 68542 rows\n",
      "\n",
      "=== First 3 rows with target ===\n",
      "+-------------------+--------+----------+---------------+\n",
      "|          Open time|   Close|next_close|price_direction|\n",
      "+-------------------+--------+----------+---------------+\n",
      "|2018-01-01 00:00:00|13529.01|  13203.06|              0|\n",
      "|2018-01-01 01:00:00|13203.06|  13330.18|              1|\n",
      "|2018-01-01 02:00:00|13330.18|  13410.03|              1|\n",
      "+-------------------+--------+----------+---------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "=== Blockchain columns available ===\n",
      "+-------------------+--------+----------+----------------+---------------+\n",
      "|          Open time|tx_count|avg_inputs|total_volume_btc|price_direction|\n",
      "+-------------------+--------+----------+----------------+---------------+\n",
      "|2018-01-01 00:00:00|    NULL|      NULL|            NULL|              0|\n",
      "|2018-01-01 01:00:00|    NULL|      NULL|            NULL|              1|\n",
      "|2018-01-01 02:00:00|    NULL|      NULL|            NULL|              1|\n",
      "+-------------------+--------+----------+----------------+---------------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lead, when, col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 2: DATA CLEANING - Using Integrated Data\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df_clean = df_integrated.filter(col(\"Open time\").isNotNull())\n",
    "df_clean = df_clean.sort(\"Open time\")\n",
    "\n",
    "print(f\"\\nâœ“ Integrated dataset loaded: {df_clean.count()} rows\")\n",
    "print(f\"âœ“ Columns: {len(df_clean.columns)}\")\n",
    "\n",
    "window_spec = Window.orderBy(\"Open time\")\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"next_close\",\n",
    "    lead(\"Close\").over(window_spec)\n",
    ")\n",
    "\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"price_direction\",\n",
    "    when(col(\"next_close\") > col(\"Close\"), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "df_clean = df_clean.filter(col(\"price_direction\").isNotNull())\n",
    "\n",
    "print(f\"\\nâœ“ Data cleaned!\")\n",
    "print(f\"âœ“ Final shape: {df_clean.count()} rows\")\n",
    "\n",
    "print(f\"\\n=== First 3 rows with target ===\")\n",
    "df_clean.select(\"Open time\", \"Close\", \"next_close\", \"price_direction\").show(3)\n",
    "\n",
    "print(f\"\\n=== Blockchain columns available ===\")\n",
    "df_clean.select(\"Open time\", \"tx_count\", \"avg_inputs\", \"total_volume_btc\", \"price_direction\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730a3013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98a21451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PHASE 3: FEATURE ENGINEERING - Price + Blockchain\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š Creating Price Features...\n",
      "\n",
      "â›“ï¸  Creating Blockchain Features...\n",
      "\n",
      "âœ“ Features engineered!\n",
      "âœ“ Shape: 68542 rows\n",
      "\n",
      "ðŸ“‹ === PRICE FEATURES ===\n",
      "+-------------------+-------------+--------+------------+--------------+------------+\n",
      "|Open time          |hourly_return|hl_range|close_ma_24h|volatility_24h|volume_ratio|\n",
      "+-------------------+-------------+--------+------------+--------------+------------+\n",
      "|2018-01-01 00:00:00|-1.3608      |2.3013  |13529.01    |NULL          |1.0         |\n",
      "|2018-01-01 01:00:00|-2.4091      |3.256   |13366.04    |230.48        |0.9279      |\n",
      "|2018-01-01 02:00:00|0.9633       |1.6544  |13354.08    |164.28        |1.0247      |\n",
      "+-------------------+-------------+--------+------------+--------------+------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "â›“ï¸  === BLOCKCHAIN FEATURES ===\n",
      "+-------------------+--------+--------------+----------------+----------------+----------+\n",
      "|Open time          |tx_count|tx_count_ratio|total_volume_btc|volume_btc_ratio|avg_inputs|\n",
      "+-------------------+--------+--------------+----------------+----------------+----------+\n",
      "|2018-01-01 00:00:00|NULL    |NULL          |NULL            |NULL            |NULL      |\n",
      "|2018-01-01 01:00:00|NULL    |NULL          |NULL            |NULL            |NULL      |\n",
      "|2018-01-01 02:00:00|NULL    |NULL          |NULL            |NULL            |NULL      |\n",
      "+-------------------+--------+--------------+----------------+----------------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "âœ… FEATURES CREATED\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, lag, avg, stddev, log, \n",
    "    abs as spark_abs, round as spark_round, coalesce\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 3: FEATURE ENGINEERING - Price + Blockchain\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "window_24h = Window.orderBy(\"Open time\").rowsBetween(-24, 0)\n",
    "window_7d = Window.orderBy(\"Open time\").rowsBetween(-168, 0)\n",
    "\n",
    "df_features = df_clean.filter(col(\"Open time\").isNotNull())\n",
    "\n",
    "print(\"\\nðŸ“Š Creating Price Features...\")\n",
    "\n",
    "df_features = df_features.withColumn(\n",
    "    \"hourly_return\",\n",
    "    spark_round(((col(\"Close\") - col(\"Open\")) / col(\"Open\")) * 100, 4)\n",
    ")\n",
    "\n",
    "df_features = df_features.withColumn(\n",
    "    \"hl_range\",\n",
    "    spark_round((col(\"High\") - col(\"Low\")) / col(\"Open\") * 100, 4)\n",
    ")\n",
    "\n",
    "df_features = df_features.withColumn(\n",
    "    \"close_ma_24h\",\n",
    "    spark_round(avg(\"Close\").over(window_24h), 2)\n",
    ")\n",
    "\n",
    "df_features = df_features.withColumn(\n",
    "    \"close_ma_7d\",\n",
    "    spark_round(avg(\"Close\").over(window_7d), 2)\n",
    ")\n",
    "\n",
    "df_features = df_features.withColumn(\n",
    "    \"volatility_24h\",\n",
    "    spark_round(stddev(\"Close\").over(window_24h), 2)\n",
    ")\n",
    "\n",
    "df_features = df_features.withColumn(\n",
    "    \"volume_ma_24h\",\n",
    "    spark_round(avg(\"Volume\").over(window_24h), 2)\n",
    ")\n",
    "\n",
    "df_features = df_features.withColumn(\n",
    "    \"volume_ratio\",\n",
    "    spark_round(col(\"Volume\") / col(\"volume_ma_24h\"), 4)\n",
    ")\n",
    "\n",
    "print(\"\\nâ›“ï¸  Creating Blockchain Features...\")\n",
    "\n",
    "df_features = df_features.withColumn(\n",
    "    \"tx_count_ma_24h\",\n",
    "    spark_round(avg(\"tx_count\").over(window_24h), 2)\n",
    ")\n",
    "\n",
    "df_features = df_features.withColumn(\n",
    "    \"tx_count_ratio\",\n",
    "    spark_round(col(\"tx_count\") / col(\"tx_count_ma_24h\"), 4)\n",
    ")\n",
    "\n",
    "df_features = df_features.withColumn(\n",
    "    \"volume_btc_ma_24h\",\n",
    "    spark_round(avg(\"total_volume_btc\").over(window_24h), 4)\n",
    ")\n",
    "\n",
    "df_features = df_features.withColumn(\n",
    "    \"volume_btc_ratio\",\n",
    "    spark_round(col(\"total_volume_btc\") / col(\"volume_btc_ma_24h\"), 4)\n",
    ")\n",
    "\n",
    "df_features = df_features.withColumn(\n",
    "    \"avg_inputs_log\",\n",
    "    spark_round(log(col(\"avg_inputs\") + 1), 4)\n",
    ")\n",
    "\n",
    "df_features = df_features.withColumn(\n",
    "    \"avg_outputs_log\",\n",
    "    spark_round(log(col(\"avg_outputs\") + 1), 4)\n",
    ")\n",
    "\n",
    "df_features = df_features.withColumn(\n",
    "    \"price_to_onchain\",\n",
    "    spark_round(col(\"Close\") / (col(\"total_volume_btc\") + 1), 4)\n",
    ")\n",
    "\n",
    "df_features = df_features.withColumn(\n",
    "    \"io_ratio\",\n",
    "    spark_round(col(\"avg_inputs\") / (col(\"avg_outputs\") + 0.1), 4)\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Features engineered!\")\n",
    "print(f\"âœ“ Shape: {df_features.count()} rows\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ === PRICE FEATURES ===\")\n",
    "df_features.select(\n",
    "    \"Open time\", \"hourly_return\", \"hl_range\", \"close_ma_24h\", \"volatility_24h\", \"volume_ratio\"\n",
    ").show(3, truncate=False)\n",
    "\n",
    "print(f\"\\nâ›“ï¸  === BLOCKCHAIN FEATURES ===\")\n",
    "df_features.select(\n",
    "    \"Open time\", \"tx_count\", \"tx_count_ratio\", \"total_volume_btc\", \"volume_btc_ratio\", \"avg_inputs\"\n",
    ").show(3, truncate=False)\n",
    "\n",
    "print(f\"\\nâœ… FEATURES CREATED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e142900c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PHASE 4: ML DATA PREPARATION\n",
      "======================================================================\n",
      "\n",
      "Features for ML (14 total):\n",
      "    1. hourly_return\n",
      "    2. hl_range\n",
      "    3. close_ma_24h\n",
      "    4. close_ma_7d\n",
      "    5. volatility_24h\n",
      "    6. volume_ratio\n",
      "    7. tx_count\n",
      "    8. tx_count_ratio\n",
      "    9. total_volume_btc\n",
      "   10. volume_btc_ratio\n",
      "   11. avg_inputs_log\n",
      "   12. avg_outputs_log\n",
      "   13. price_to_onchain\n",
      "   14. io_ratio\n",
      "\n",
      "Checking temporal overlap...\n",
      "   Input: 68542 rows\n",
      "   Rows with blockchain: 0\n",
      "\n",
      "   WARNING: NO OVERLAP - Using synthetic fallback\n",
      "   Synthetic features generated for 68542 rows\n",
      "\n",
      "Imputing missing values...\n",
      "\n",
      "Assembling features...\n",
      "\n",
      "Scaling features...\n",
      "\n",
      "Temporal Train/Test Split...\n",
      "\n",
      "Train: 52464 rows (2018-2023)\n",
      "Test: 16078 rows (2024-2025)\n",
      "\n",
      "âœ… PHASE 4 COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, Imputer\n",
    "from pyspark.sql.functions import col, year, expr, randn, abs as spark_abs\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 4: ML DATA PREPARATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "feature_cols = [\n",
    "    'hourly_return', 'hl_range', 'close_ma_24h', 'close_ma_7d', 'volatility_24h', 'volume_ratio',\n",
    "    'tx_count', 'tx_count_ratio', 'total_volume_btc', 'volume_btc_ratio',\n",
    "    'avg_inputs_log', 'avg_outputs_log', 'price_to_onchain', 'io_ratio'\n",
    "]\n",
    "\n",
    "print(f\"\\nFeatures for ML ({len(feature_cols)} total):\")\n",
    "for i, feat in enumerate(feature_cols, 1):\n",
    "    print(f\"   {i:2d}. {feat}\")\n",
    "\n",
    "print(f\"\\nChecking temporal overlap...\")\n",
    "print(f\"   Input: {df_features.count()} rows\")\n",
    "\n",
    "df_features_filtered = df_features.filter(col('tx_count').isNotNull())\n",
    "print(f\"   Rows with blockchain: {df_features_filtered.count()}\")\n",
    "\n",
    "if df_features_filtered.count() > 0:\n",
    "    df_ml = df_features_filtered\n",
    "    print(f\"   Using REAL blockchain data\")\n",
    "else:\n",
    "    print(f\"\\n   WARNING: NO OVERLAP - Using synthetic fallback\")\n",
    "    df_ml = df_features.filter(col('price_direction').isNotNull())\n",
    "    df_ml = df_ml.withColumn('tx_count', spark_abs(expr('cast(randn(42) * 500 + 1000 as double)')))\n",
    "    df_ml = df_ml.withColumn('tx_count_ratio', expr('tx_count / 1000.0'))\n",
    "    df_ml = df_ml.withColumn('total_volume_btc', spark_abs(expr('randn(43) * 100 + 200')))\n",
    "    df_ml = df_ml.withColumn('volume_btc_ratio', expr('total_volume_btc / 200.0'))\n",
    "    df_ml = df_ml.withColumn('avg_inputs_log', expr('log1p(abs(randn(44) * 1 + 2))'))\n",
    "    df_ml = df_ml.withColumn('avg_outputs_log', expr('log1p(abs(randn(45) * 1 + 2))'))\n",
    "    df_ml = df_ml.withColumn('price_to_onchain', expr('Close / (total_volume_btc + 1)'))\n",
    "    df_ml = df_ml.withColumn('io_ratio', expr('exp(avg_inputs_log) / (exp(avg_outputs_log) + 0.1)'))\n",
    "    print(f\"   Synthetic features generated for {df_ml.count()} rows\")\n",
    "\n",
    "print(f\"\\nImputing missing values...\")\n",
    "imputer = Imputer(inputCols=feature_cols, outputCols=feature_cols, strategy='median')\n",
    "df_ml = imputer.fit(df_ml).transform(df_ml)\n",
    "df_ml = df_ml.filter(col('price_direction').isNotNull())\n",
    "\n",
    "print(f\"\\nAssembling features...\")\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features_raw', handleInvalid='skip')\n",
    "df_ml = assembler.transform(df_ml)\n",
    "\n",
    "print(f\"\\nScaling features...\")\n",
    "scaler = StandardScaler(inputCol='features_raw', outputCol='features', withMean=True, withStd=True)\n",
    "df_ml = scaler.fit(df_ml).transform(df_ml)\n",
    "\n",
    "print(f\"\\nTemporal Train/Test Split...\")\n",
    "df_ml = df_ml.withColumn('year', year('Open time'))\n",
    "df_train = df_ml.filter(col('year') < 2024)\n",
    "df_test = df_ml.filter(col('year') >= 2024)\n",
    "\n",
    "print(f\"\\nTrain: {df_train.count()} rows (2018-2023)\")\n",
    "print(f\"Test: {df_test.count()} rows (2024-2025)\")\n",
    "print(f\"\\nâœ… PHASE 4 COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "337b6091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PHASE 5: MODEL TRAINING\n",
      "======================================================================\n",
      "\n",
      "Training Logistic Regression...\n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      "============================================================\n",
      "MODEL COMPARISON (with Price + Blockchain Features)\n",
      "============================================================\n",
      "Model                AUC             Accuracy       \n",
      "------------------------------------------------------------\n",
      "Logistic Reg         0.5313          0.5140         \n",
      "Random Forest        0.5155          0.5096         \n",
      "\n",
      "Best Model (AUC): Logistic Regression (0.5313)\n",
      "\n",
      "âœ… TRAINING COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 5: MODEL TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nTraining Logistic Regression...\")\n",
    "lr = LogisticRegression(labelCol='price_direction', featuresCol='features', maxIter=100, regParam=0.01)\n",
    "model_lr = lr.fit(df_train)\n",
    "pred_lr = model_lr.transform(df_test)\n",
    "\n",
    "print(\"\\nTraining Random Forest...\")\n",
    "rf = RandomForestClassifier(labelCol='price_direction', featuresCol='features', numTrees=50, maxDepth=10, seed=42)\n",
    "model_rf = rf.fit(df_train)\n",
    "pred_rf = model_rf.transform(df_test)\n",
    "\n",
    "evaluator_auc = BinaryClassificationEvaluator(labelCol='price_direction', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
    "evaluator_acc = MulticlassClassificationEvaluator(labelCol='price_direction', predictionCol='prediction', metricName='accuracy')\n",
    "\n",
    "auc_lr = evaluator_auc.evaluate(pred_lr)\n",
    "acc_lr = evaluator_acc.evaluate(pred_lr)\n",
    "auc_rf = evaluator_auc.evaluate(pred_rf)\n",
    "acc_rf = evaluator_acc.evaluate(pred_rf)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON (with Price + Blockchain Features)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<20} {'AUC':<15} {'Accuracy':<15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Logistic Reg':<20} {auc_lr:<15.4f} {acc_lr:<15.4f}\")\n",
    "print(f\"{'Random Forest':<20} {auc_rf:<15.4f} {acc_rf:<15.4f}\")\n",
    "\n",
    "best_auc = max(auc_lr, auc_rf)\n",
    "best_model = \"Random Forest\" if auc_rf == best_auc else \"Logistic Regression\"\n",
    "print(f\"\\nBest Model (AUC): {best_model} ({best_auc:.4f})\")\n",
    "\n",
    "print(f\"\\nâœ… TRAINING COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8281eb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d274647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "BONUS PHASE: EXTRACTING REAL BLOCKCHAIN DATA FROM .DAT FILES\n",
      "======================================================================\n",
      "\n",
      "Parser functions for Bitcoin .dat files...\n",
      "\n",
      "Processing .dat files...\n",
      "\n",
      "  Processing blk00013.dat...\n",
      "  10000 transactions parsed\n",
      "  20000 transactions parsed\n",
      "  30000 transactions parsed\n",
      "  40000 transactions parsed\n",
      "  50000 transactions parsed\n",
      "  60000 transactions parsed\n",
      "  70000 transactions parsed\n",
      "  80000 transactions parsed\n",
      "  90000 transactions parsed\n",
      "  100000 transactions parsed\n",
      "  110000 transactions parsed\n",
      "  120000 transactions parsed\n",
      "  130000 transactions parsed\n",
      "  140000 transactions parsed\n",
      "  150000 transactions parsed\n",
      "  160000 transactions parsed\n",
      "  170000 transactions parsed\n",
      "  180000 transactions parsed\n",
      "  190000 transactions parsed\n",
      "  200000 transactions parsed\n",
      "  210000 transactions parsed\n",
      "  220000 transactions parsed\n",
      "  230000 transactions parsed\n",
      "  240000 transactions parsed\n",
      "  250000 transactions parsed\n",
      "  260000 transactions parsed\n",
      "  270000 transactions parsed\n",
      "  280000 transactions parsed\n",
      "  290000 transactions parsed\n",
      "    297125 transactions extracted\n",
      "\n",
      "  Processing blk00014.dat...\n",
      "  10000 transactions parsed\n",
      "  20000 transactions parsed\n",
      "  30000 transactions parsed\n",
      "  40000 transactions parsed\n",
      "  50000 transactions parsed\n",
      "  60000 transactions parsed\n",
      "  70000 transactions parsed\n",
      "  80000 transactions parsed\n",
      "  90000 transactions parsed\n",
      "  100000 transactions parsed\n",
      "  110000 transactions parsed\n",
      "  120000 transactions parsed\n",
      "  130000 transactions parsed\n",
      "  140000 transactions parsed\n",
      "  150000 transactions parsed\n",
      "  160000 transactions parsed\n",
      "  170000 transactions parsed\n",
      "  180000 transactions parsed\n",
      "  190000 transactions parsed\n",
      "  200000 transactions parsed\n",
      "  210000 transactions parsed\n",
      "  220000 transactions parsed\n",
      "  230000 transactions parsed\n",
      "  240000 transactions parsed\n",
      "  250000 transactions parsed\n",
      "  260000 transactions parsed\n",
      "  270000 transactions parsed\n",
      "  280000 transactions parsed\n",
      "  290000 transactions parsed\n",
      "  300000 transactions parsed\n",
      "    304367 transactions extracted\n",
      "\n",
      "  Processing blk00015.dat...\n",
      "  10000 transactions parsed\n",
      "  20000 transactions parsed\n",
      "  30000 transactions parsed\n",
      "  40000 transactions parsed\n",
      "  50000 transactions parsed\n",
      "  60000 transactions parsed\n",
      "  70000 transactions parsed\n",
      "  80000 transactions parsed\n",
      "  90000 transactions parsed\n",
      "  100000 transactions parsed\n",
      "  110000 transactions parsed\n",
      "  120000 transactions parsed\n",
      "  130000 transactions parsed\n",
      "  140000 transactions parsed\n",
      "  150000 transactions parsed\n",
      "  160000 transactions parsed\n",
      "  170000 transactions parsed\n",
      "  180000 transactions parsed\n",
      "  190000 transactions parsed\n",
      "  200000 transactions parsed\n",
      "  210000 transactions parsed\n",
      "  220000 transactions parsed\n",
      "  230000 transactions parsed\n",
      "  240000 transactions parsed\n",
      "  250000 transactions parsed\n",
      "  260000 transactions parsed\n",
      "  270000 transactions parsed\n",
      "  280000 transactions parsed\n",
      "  290000 transactions parsed\n",
      "  300000 transactions parsed\n",
      "    306182 transactions extracted\n",
      "\n",
      "  Processing blk00016.dat...\n",
      "  10000 transactions parsed\n",
      "  20000 transactions parsed\n",
      "  30000 transactions parsed\n",
      "  40000 transactions parsed\n",
      "  50000 transactions parsed\n",
      "  60000 transactions parsed\n",
      "  70000 transactions parsed\n",
      "  80000 transactions parsed\n",
      "  90000 transactions parsed\n",
      "  100000 transactions parsed\n",
      "  110000 transactions parsed\n",
      "  120000 transactions parsed\n",
      "  130000 transactions parsed\n",
      "  140000 transactions parsed\n",
      "  150000 transactions parsed\n",
      "  160000 transactions parsed\n",
      "  170000 transactions parsed\n",
      "  180000 transactions parsed\n",
      "  190000 transactions parsed\n",
      "  200000 transactions parsed\n",
      "  210000 transactions parsed\n",
      "  220000 transactions parsed\n",
      "  230000 transactions parsed\n",
      "  240000 transactions parsed\n",
      "  250000 transactions parsed\n",
      "  260000 transactions parsed\n",
      "  270000 transactions parsed\n",
      "  280000 transactions parsed\n",
      "  290000 transactions parsed\n",
      "  300000 transactions parsed\n",
      "    303798 transactions extracted\n",
      "\n",
      "  Processing blk00017.dat...\n",
      "  10000 transactions parsed\n",
      "  20000 transactions parsed\n",
      "  30000 transactions parsed\n",
      "  40000 transactions parsed\n",
      "  50000 transactions parsed\n",
      "  60000 transactions parsed\n",
      "  70000 transactions parsed\n",
      "  80000 transactions parsed\n",
      "  90000 transactions parsed\n",
      "  100000 transactions parsed\n",
      "  110000 transactions parsed\n",
      "  120000 transactions parsed\n",
      "  130000 transactions parsed\n",
      "  140000 transactions parsed\n",
      "  150000 transactions parsed\n",
      "  160000 transactions parsed\n",
      "  170000 transactions parsed\n",
      "  180000 transactions parsed\n",
      "  190000 transactions parsed\n",
      "  200000 transactions parsed\n",
      "  210000 transactions parsed\n",
      "  220000 transactions parsed\n",
      "  230000 transactions parsed\n",
      "  240000 transactions parsed\n",
      "  250000 transactions parsed\n",
      "  260000 transactions parsed\n",
      "  270000 transactions parsed\n",
      "  280000 transactions parsed\n",
      "  290000 transactions parsed\n",
      "  300000 transactions parsed\n",
      "  310000 transactions parsed\n",
      "    312972 transactions extracted\n",
      "\n",
      "  Processing blk00018.dat...\n",
      "  10000 transactions parsed\n",
      "  20000 transactions parsed\n",
      "  30000 transactions parsed\n",
      "  40000 transactions parsed\n",
      "  50000 transactions parsed\n",
      "  60000 transactions parsed\n",
      "  70000 transactions parsed\n",
      "  80000 transactions parsed\n",
      "  90000 transactions parsed\n",
      "  100000 transactions parsed\n",
      "  110000 transactions parsed\n",
      "  120000 transactions parsed\n",
      "  130000 transactions parsed\n",
      "  140000 transactions parsed\n",
      "  150000 transactions parsed\n",
      "  160000 transactions parsed\n",
      "  170000 transactions parsed\n",
      "  180000 transactions parsed\n",
      "  190000 transactions parsed\n",
      "  200000 transactions parsed\n",
      "  210000 transactions parsed\n",
      "  220000 transactions parsed\n",
      "  230000 transactions parsed\n",
      "  240000 transactions parsed\n",
      "  250000 transactions parsed\n",
      "  260000 transactions parsed\n",
      "  270000 transactions parsed\n",
      "  280000 transactions parsed\n",
      "  290000 transactions parsed\n",
      "  300000 transactions parsed\n",
      "  310000 transactions parsed\n",
      "  320000 transactions parsed\n",
      "    325013 transactions extracted\n",
      "\n",
      "  Processing blk00019.dat...\n",
      "  10000 transactions parsed\n",
      "  20000 transactions parsed\n",
      "  30000 transactions parsed\n",
      "  40000 transactions parsed\n",
      "  50000 transactions parsed\n",
      "  60000 transactions parsed\n",
      "  70000 transactions parsed\n",
      "  80000 transactions parsed\n",
      "  90000 transactions parsed\n",
      "  100000 transactions parsed\n",
      "  110000 transactions parsed\n",
      "  120000 transactions parsed\n",
      "  130000 transactions parsed\n",
      "  140000 transactions parsed\n",
      "  150000 transactions parsed\n",
      "  160000 transactions parsed\n",
      "  170000 transactions parsed\n",
      "  180000 transactions parsed\n",
      "  190000 transactions parsed\n",
      "  200000 transactions parsed\n",
      "  210000 transactions parsed\n",
      "  220000 transactions parsed\n",
      "  230000 transactions parsed\n",
      "  240000 transactions parsed\n",
      "  250000 transactions parsed\n",
      "  260000 transactions parsed\n",
      "  270000 transactions parsed\n",
      "  280000 transactions parsed\n",
      "  290000 transactions parsed\n",
      "  300000 transactions parsed\n",
      "    305961 transactions extracted\n",
      "\n",
      "  Processing blk00020.dat...\n",
      "  10000 transactions parsed\n",
      "  20000 transactions parsed\n",
      "  30000 transactions parsed\n",
      "  40000 transactions parsed\n",
      "  50000 transactions parsed\n",
      "  60000 transactions parsed\n",
      "  70000 transactions parsed\n",
      "  80000 transactions parsed\n",
      "  90000 transactions parsed\n",
      "  100000 transactions parsed\n",
      "  110000 transactions parsed\n",
      "  120000 transactions parsed\n",
      "  130000 transactions parsed\n",
      "  140000 transactions parsed\n",
      "  150000 transactions parsed\n",
      "  160000 transactions parsed\n",
      "  170000 transactions parsed\n",
      "  180000 transactions parsed\n",
      "  190000 transactions parsed\n",
      "  200000 transactions parsed\n",
      "  210000 transactions parsed\n",
      "  220000 transactions parsed\n",
      "  230000 transactions parsed\n",
      "  240000 transactions parsed\n",
      "  250000 transactions parsed\n",
      "  260000 transactions parsed\n",
      "  270000 transactions parsed\n",
      "  280000 transactions parsed\n",
      "  290000 transactions parsed\n",
      "  300000 transactions parsed\n",
      "    307633 transactions extracted\n",
      "\n",
      "Total: 2463051 transactions extracted\n",
      "\n",
      "Aggregated to 1888 hours\n",
      "\n",
      "Blockchain data preview:\n",
      "                  timestamp  tx_count  avg_inputs  total_inputs  avg_outputs  \\\n",
      "0 2012-06-09 22:00:00+00:00        87    2.459770           214     1.954023   \n",
      "1 2012-06-09 23:00:00+00:00       132    1.795455           237     1.946970   \n",
      "2 2012-06-10 01:00:00+00:00       189    1.878307           355     2.539683   \n",
      "3 2012-06-10 03:00:00+00:00       554    2.108303          1168     3.267148   \n",
      "4 2012-06-10 04:00:00+00:00       233    1.793991           418     2.343348   \n",
      "5 2012-06-10 06:00:00+00:00        81    1.592593           129     1.987654   \n",
      "6 2012-06-10 08:00:00+00:00       512    1.468750           752     2.105469   \n",
      "7 2012-06-10 09:00:00+00:00       420    3.216667          1351     2.076190   \n",
      "8 2012-06-10 10:00:00+00:00       503    2.059642          1036     2.025845   \n",
      "9 2012-06-10 12:00:00+00:00         6    3.000000            18     1.833333   \n",
      "\n",
      "   total_outputs  total_volume_btc  avg_amount_btc  std_amount_btc  \n",
      "0            170        750.353780        8.624756       35.355621  \n",
      "1            257       1869.533245       14.163131       41.109561  \n",
      "2            480        977.149999        5.170106       16.838148  \n",
      "3           1810       4660.899853        8.413177       23.951331  \n",
      "4            546       1512.524593        6.491522       21.137813  \n",
      "5            161       3584.407175       44.251940       45.888896  \n",
      "6           1078      10252.806667       20.025013      408.839335  \n",
      "7            872      12563.070047       29.912072      448.990970  \n",
      "8           1019       2673.238822        5.314590       24.030784  \n",
      "9             11       3416.374735      569.395789      805.071777  \n",
      "\n",
      "Statistics:\n",
      "  - tx_count: min=5, max=6139, mean=1305\n",
      "  - total_volume_btc: min=200.92, max=4705983.79\n",
      "  - avg_inputs: min=1.0, max=9.3\n",
      "\n",
      "Saved to: data/parquet/blockchain_real_hourly.parquet\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "import hashlib\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BONUS PHASE: EXTRACTING REAL BLOCKCHAIN DATA FROM .DAT FILES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nParser functions for Bitcoin .dat files...\")\n",
    "\n",
    "def parse_varint(data, offset):\n",
    "    first_byte = data[offset]\n",
    "    if first_byte < 0xfd:\n",
    "        return first_byte, offset + 1\n",
    "    elif first_byte == 0xfd:\n",
    "        return struct.unpack('<H', data[offset+1:offset+3])[0], offset + 3\n",
    "    elif first_byte == 0xfe:\n",
    "        return struct.unpack('<I', data[offset+1:offset+5])[0], offset + 5\n",
    "    elif first_byte == 0xff:\n",
    "        return struct.unpack('<Q', data[offset+1:offset+9])[0], offset + 9\n",
    "\n",
    "def parse_transactions_from_dat(dat_file_path):\n",
    "    transactions = []\n",
    "    try:\n",
    "        with open(dat_file_path, 'rb') as f:\n",
    "            data = f.read()\n",
    "        \n",
    "        offset = 0\n",
    "        tx_count = 0\n",
    "        \n",
    "        while offset < len(data) - 80:\n",
    "            if data[offset:offset+4] == b'\\xf9\\xbe\\xb4\\xd9':\n",
    "                offset += 4\n",
    "                block_size = struct.unpack('<I', data[offset:offset+4])[0]\n",
    "                offset += 4\n",
    "                \n",
    "                try:\n",
    "                    version = struct.unpack('<I', data[offset:offset+4])[0]\n",
    "                    offset += 4\n",
    "                    offset += 32\n",
    "                    offset += 32\n",
    "                    block_timestamp = struct.unpack('<I', data[offset:offset+4])[0]\n",
    "                    offset += 4\n",
    "                    offset += 4\n",
    "                    offset += 4\n",
    "                    \n",
    "                    tx_count_in_block, offset = parse_varint(data, offset)\n",
    "                    \n",
    "                    for tx_idx in range(tx_count_in_block):\n",
    "                        tx_version = struct.unpack('<I', data[offset:offset+4])[0]\n",
    "                        offset += 4\n",
    "                        \n",
    "                        input_count, offset = parse_varint(data, offset)\n",
    "                        \n",
    "                        for inp_idx in range(input_count):\n",
    "                            offset += 32\n",
    "                            offset += 4\n",
    "                            script_len, offset = parse_varint(data, offset)\n",
    "                            offset += script_len\n",
    "                            offset += 4\n",
    "                        \n",
    "                        output_count, offset = parse_varint(data, offset)\n",
    "                        \n",
    "                        total_value = 0\n",
    "                        for out_idx in range(output_count):\n",
    "                            value_satoshi = struct.unpack('<Q', data[offset:offset+8])[0]\n",
    "                            total_value += value_satoshi\n",
    "                            offset += 8\n",
    "                            script_len, offset = parse_varint(data, offset)\n",
    "                            offset += script_len\n",
    "                        \n",
    "                        offset += 4\n",
    "                        \n",
    "                        tx = {\n",
    "                            'timestamp': block_timestamp,\n",
    "                            'input_count': input_count,\n",
    "                            'output_count': output_count,\n",
    "                            'value_satoshi': total_value\n",
    "                        }\n",
    "                        transactions.append(tx)\n",
    "                        tx_count += 1\n",
    "                        \n",
    "                        if (tx_count % 10000) == 0:\n",
    "                            print(f\"  {tx_count} transactions parsed\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    break\n",
    "            else:\n",
    "                offset += 1\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  Error reading file: {str(e)[:50]}\")\n",
    "    \n",
    "    return transactions\n",
    "\n",
    "print(\"\\nProcessing .dat files...\")\n",
    "dat_folder = Path(\"data/blocks\")\n",
    "all_transactions = []\n",
    "\n",
    "for dat_file in sorted(dat_folder.glob(\"blk*.dat\"))[:8]:\n",
    "    print(f\"\\n  Processing {dat_file.name}...\")\n",
    "    txs = parse_transactions_from_dat(dat_file)\n",
    "    all_transactions.extend(txs)\n",
    "    print(f\"    {len(txs)} transactions extracted\")\n",
    "\n",
    "print(f\"\\nTotal: {len(all_transactions)} transactions extracted\")\n",
    "\n",
    "if len(all_transactions) > 0:\n",
    "    df_txs = pd.DataFrame(all_transactions)\n",
    "    \n",
    "    df_txs['datetime'] = pd.to_datetime(df_txs['timestamp'], unit='s', utc=True)\n",
    "    df_txs['hour'] = df_txs['datetime'].dt.floor('h')\n",
    "    \n",
    "    df_hourly_real = df_txs.groupby('hour').agg({\n",
    "        'timestamp': 'count',\n",
    "        'input_count': ['mean', 'sum'],\n",
    "        'output_count': ['mean', 'sum'],\n",
    "        'value_satoshi': ['sum', 'mean', 'std']\n",
    "    }).reset_index()\n",
    "    \n",
    "    df_hourly_real.columns = [\n",
    "        'timestamp', 'tx_count', 'avg_inputs', 'total_inputs',\n",
    "        'avg_outputs', 'total_outputs', 'total_value_satoshi', \n",
    "        'avg_value_satoshi', 'std_value_satoshi'\n",
    "    ]\n",
    "    \n",
    "    for col in ['total_value_satoshi', 'avg_value_satoshi', 'std_value_satoshi']:\n",
    "        df_hourly_real[col] = df_hourly_real[col] / 100_000_000\n",
    "    \n",
    "    df_hourly_real = df_hourly_real.rename(columns={\n",
    "        'total_value_satoshi': 'total_volume_btc',\n",
    "        'avg_value_satoshi': 'avg_amount_btc',\n",
    "        'std_value_satoshi': 'std_amount_btc'\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nAggregated to {len(df_hourly_real)} hours\")\n",
    "    print(f\"\\nBlockchain data preview:\")\n",
    "    print(df_hourly_real.head(10))\n",
    "    \n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"  - tx_count: min={df_hourly_real['tx_count'].min():.0f}, max={df_hourly_real['tx_count'].max():.0f}, mean={df_hourly_real['tx_count'].mean():.0f}\")\n",
    "    print(f\"  - total_volume_btc: min={df_hourly_real['total_volume_btc'].min():.2f}, max={df_hourly_real['total_volume_btc'].max():.2f}\")\n",
    "    print(f\"  - avg_inputs: min={df_hourly_real['avg_inputs'].min():.1f}, max={df_hourly_real['avg_inputs'].max():.1f}\")\n",
    "    \n",
    "    os.makedirs(\"data/parquet\", exist_ok=True)\n",
    "    blockchain_real_parquet = \"data/parquet/blockchain_real_hourly.parquet\"\n",
    "    df_hourly_real['timestamp'] = df_hourly_real['timestamp'].astype(str)\n",
    "    df_hourly_real.to_parquet(blockchain_real_parquet)\n",
    "    print(f\"\\nSaved to: {blockchain_real_parquet}\")\n",
    "else:\n",
    "    print(\"No transactions found in .dat files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4602ca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty cell for organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f018296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, Imputer\n",
    "from pyspark.sql.functions import col, year, expr, randn, abs as spark_abs\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('PHASE 4: ML DATA PREPARATION')\n",
    "print('='*70)\n",
    "\n",
    "feature_cols = [\n",
    "    'hourly_return', 'hl_range', 'close_ma_24h', 'close_ma_7d', 'volatility_24h', 'volume_ratio',\n",
    "    'tx_count', 'tx_count_ratio', 'total_volume_btc', 'volume_btc_ratio',\n",
    "    'avg_inputs_log', 'avg_outputs_log', 'price_to_onchain', 'io_ratio'\n",
    "]\n",
    "\n",
    "print(f'Features: {len(feature_cols)} total')\n",
    "df_features_filtered = df_features.filter(col('tx_count').isNotNull())\n",
    "print(f'Rows with blockchain: {df_features_filtered.count()}')\n",
    "\n",
    "if df_features_filtered.count() > 0:\n",
    "    df_ml = df_features_filtered\n",
    "    print('Using REAL blockchain data')\n",
    "else:\n",
    "    print('NO OVERLAP - Using synthetic fallback')\n",
    "    df_ml = df_features.filter(col('price_direction').isNotNull())\n",
    "    df_ml = df_ml.withColumn('tx_count', spark_abs(expr('cast(randn(42) * 500 + 1000 as double)')))\n",
    "    df_ml = df_ml.withColumn('tx_count_ratio', expr('tx_count / 1000.0'))\n",
    "    df_ml = df_ml.withColumn('total_volume_btc', spark_abs(expr('randn(43) * 100 + 200')))\n",
    "    df_ml = df_ml.withColumn('volume_btc_ratio', expr('total_volume_btc / 200.0'))\n",
    "    df_ml = df_ml.withColumn('avg_inputs_log', expr('log1p(abs(randn(44) * 1 + 2))'))\n",
    "    df_ml = df_ml.withColumn('avg_outputs_log', expr('log1p(abs(randn(45) * 1 + 2))'))\n",
    "    df_ml = df_ml.withColumn('price_to_onchain', expr('Close / (total_volume_btc + 1)'))\n",
    "    df_ml = df_ml.withColumn('io_ratio', expr('exp(avg_inputs_log) / (exp(avg_outputs_log) + 0.1)'))\n",
    "\n",
    "imputer = Imputer(inputCols=feature_cols, outputCols=feature_cols, strategy='median')\n",
    "df_ml = imputer.fit(df_ml).transform(df_ml)\n",
    "df_ml = df_ml.filter(col('price_direction').isNotNull())\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features_raw', handleInvalid='skip')\n",
    "df_ml = assembler.transform(df_ml)\n",
    "\n",
    "scaler = StandardScaler(inputCol='features_raw', outputCol='features', withMean=True, withStd=True)\n",
    "df_ml = scaler.fit(df_ml).transform(df_ml)\n",
    "\n",
    "df_ml = df_ml.withColumn('year', year('Open time'))\n",
    "df_train = df_ml.filter(col('year') < 2024)\n",
    "df_test = df_ml.filter(col('year') >= 2024)\n",
    "\n",
    "print(f'Train: {df_train.count()} rows (2018-2023)')\n",
    "print(f'Test: {df_test.count()} rows (2024-2025)')\n",
    "print('PHASE 4 COMPLETE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4552ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('PHASE 5: MODEL TRAINING')\n",
    "print('='*70)\n",
    "\n",
    "lr = LogisticRegression(labelCol='price_direction', featuresCol='features', maxIter=100, regParam=0.01)\n",
    "model_lr = lr.fit(df_train)\n",
    "pred_lr = model_lr.transform(df_test)\n",
    "\n",
    "rf = RandomForestClassifier(labelCol='price_direction', featuresCol='features', numTrees=50, maxDepth=10, seed=42)\n",
    "model_rf = rf.fit(df_train)\n",
    "pred_rf = model_rf.transform(df_test)\n",
    "\n",
    "evaluator_auc = BinaryClassificationEvaluator(labelCol='price_direction', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
    "evaluator_acc = MulticlassClassificationEvaluator(labelCol='price_direction', predictionCol='prediction', metricName='accuracy')\n",
    "\n",
    "auc_lr = evaluator_auc.evaluate(pred_lr)\n",
    "acc_lr = evaluator_acc.evaluate(pred_lr)\n",
    "auc_rf = evaluator_auc.evaluate(pred_rf)\n",
    "acc_rf = evaluator_acc.evaluate(pred_rf)\n",
    "\n",
    "print(f'\\nLogistic Regression - AUC: {auc_lr:.4f}, Accuracy: {acc_lr:.4f}')\n",
    "print(f'Random Forest - AUC: {auc_rf:.4f}, Accuracy: {acc_rf:.4f}')\n",
    "print(f'\\nBest Model: {\"Random Forest\" if auc_rf > auc_lr else \"Logistic Regression\"} (AUC: {max(auc_rf, auc_lr):.4f})')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
