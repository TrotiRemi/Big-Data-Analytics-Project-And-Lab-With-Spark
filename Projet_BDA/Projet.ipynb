{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a795f003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.1\n",
      "PySpark version: 4.0.1\n",
      "Python version: 3.10.19\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import platform\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName('BDA-Project')\n",
    "    .config('spark.sql.session.timeZone', 'UTC')\n",
    "    .config('spark.sql.shuffle.partitions', '4')\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel('WARN')\n",
    "\n",
    "print(f'Spark version: {spark.version}')\n",
    "print(f'PySpark version: {pyspark.__version__}')\n",
    "print(f'Python version: {sys.version.split()[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5c342a",
   "metadata": {},
   "source": [
    "# BONUS PHASE: Extract Real Blockchain Data\n",
    "**MUST RUN FIRST** - Extracts real Bitcoin transactions for maximum model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f29f198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "BONUS PHASE: EXTRACTING REAL BLOCKCHAIN DATA FROM .DAT FILES\n",
      "======================================================================\n",
      "\n",
      "üìñ Step 1: Parsing .dat files...\n",
      "  Found 8 .dat files\n",
      "\n",
      "  [1] Processing blk00013.dat...\n",
      "  Extracted 100,000 transactions...\n",
      "  Extracted 200,000 transactions...\n",
      "      Extracted 297,125 transactions (Total: 297,125)\n",
      "\n",
      "  [2] Processing blk00014.dat...\n",
      "  Extracted 100,000 transactions...\n",
      "  Extracted 200,000 transactions...\n",
      "  Extracted 300,000 transactions...\n",
      "      Extracted 304,367 transactions (Total: 601,492)\n",
      "\n",
      "  [3] Processing blk00015.dat...\n",
      "  Extracted 100,000 transactions...\n",
      "  Extracted 200,000 transactions...\n",
      "  Extracted 300,000 transactions...\n",
      "      Extracted 306,182 transactions (Total: 907,674)\n",
      "\n",
      "  [4] Processing blk00016.dat...\n",
      "  Extracted 100,000 transactions...\n",
      "  Extracted 200,000 transactions...\n",
      "  Extracted 300,000 transactions...\n",
      "      Extracted 303,798 transactions (Total: 1,211,472)\n",
      "\n",
      "  [5] Processing blk00017.dat...\n",
      "  Extracted 100,000 transactions...\n",
      "  Extracted 200,000 transactions...\n",
      "  Extracted 300,000 transactions...\n",
      "      Extracted 312,972 transactions (Total: 1,524,444)\n",
      "\n",
      "  [6] Processing blk00018.dat...\n",
      "  Extracted 100,000 transactions...\n",
      "  Extracted 200,000 transactions...\n",
      "  Extracted 300,000 transactions...\n",
      "      Extracted 325,013 transactions (Total: 1,849,457)\n",
      "\n",
      "  [7] Processing blk00019.dat...\n",
      "  Extracted 100,000 transactions...\n",
      "  Extracted 200,000 transactions...\n",
      "  Extracted 300,000 transactions...\n",
      "      Extracted 305,961 transactions (Total: 2,155,418)\n",
      "\n",
      "  [8] Processing blk00020.dat...\n",
      "  Extracted 100,000 transactions...\n",
      "  Extracted 200,000 transactions...\n",
      "  Extracted 300,000 transactions...\n",
      "      Extracted 307,633 transactions (Total: 2,463,051)\n",
      "\n",
      "‚úÖ TOTAL: 2,463,051 real blockchain transactions extracted\n",
      "\n",
      "üìä Step 2: Aggregating by hour...\n",
      "  Aggregated to 1,888 hourly records\n",
      "  Period: 2012-06-09 22:00:00+00:00 to 2012-08-30 09:00:00+00:00\n",
      "\n",
      "üìã Statistics:\n",
      "  - tx_count: min=5, max=6139, mean=1305\n",
      "  - total_volume_btc: min=200.92, max=4705983.79\n",
      "  - avg_inputs: min=1.0, max=9.3\n",
      "\n",
      "üíæ Step 3: Saving to Parquet...\n",
      "  Saved: data/parquet/blockchain_real_hourly.parquet\n",
      "\n",
      "‚úÖ BONUS PHASE COMPLETE - Real blockchain data ready!\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BONUS PHASE: EXTRACTING REAL BLOCKCHAIN DATA FROM .DAT FILES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def parse_varint(data, offset):\n",
    "    \"\"\"Parse Bitcoin varint (variable-length integer)\"\"\"\n",
    "    first_byte = data[offset]\n",
    "    if first_byte < 0xfd:\n",
    "        return first_byte, offset + 1\n",
    "    elif first_byte == 0xfd:\n",
    "        return struct.unpack('<H', data[offset+1:offset+3])[0], offset + 3\n",
    "    elif first_byte == 0xfe:\n",
    "        return struct.unpack('<I', data[offset+1:offset+5])[0], offset + 5\n",
    "    elif first_byte == 0xff:\n",
    "        return struct.unpack('<Q', data[offset+1:offset+9])[0], offset + 9\n",
    "\n",
    "def parse_transactions_from_dat(dat_file_path):\n",
    "    \"\"\"Extract transactions from Bitcoin Core .dat binary format\"\"\"\n",
    "    transactions = []\n",
    "    try:\n",
    "        with open(dat_file_path, 'rb') as f:\n",
    "            data = f.read()\n",
    "        \n",
    "        offset = 0\n",
    "        tx_count = 0\n",
    "        \n",
    "        while offset < len(data) - 80:\n",
    "            if data[offset:offset+4] == b'\\xf9\\xbe\\xb4\\xd9':\n",
    "                offset += 4\n",
    "                block_size = struct.unpack('<I', data[offset:offset+4])[0]\n",
    "                offset += 4\n",
    "                \n",
    "                try:\n",
    "                    version = struct.unpack('<I', data[offset:offset+4])[0]\n",
    "                    offset += 4\n",
    "                    offset += 32\n",
    "                    offset += 32\n",
    "                    block_timestamp = struct.unpack('<I', data[offset:offset+4])[0]\n",
    "                    offset += 4\n",
    "                    offset += 4\n",
    "                    offset += 4\n",
    "                    \n",
    "                    tx_count_in_block, offset = parse_varint(data, offset)\n",
    "                    \n",
    "                    for tx_idx in range(tx_count_in_block):\n",
    "                        tx_version = struct.unpack('<I', data[offset:offset+4])[0]\n",
    "                        offset += 4\n",
    "                        \n",
    "                        input_count, offset = parse_varint(data, offset)\n",
    "                        \n",
    "                        for inp_idx in range(input_count):\n",
    "                            offset += 32\n",
    "                            offset += 4\n",
    "                            script_len, offset = parse_varint(data, offset)\n",
    "                            offset += script_len\n",
    "                            offset += 4\n",
    "                        \n",
    "                        output_count, offset = parse_varint(data, offset)\n",
    "                        \n",
    "                        total_value = 0\n",
    "                        for out_idx in range(output_count):\n",
    "                            value_satoshi = struct.unpack('<Q', data[offset:offset+8])[0]\n",
    "                            total_value += value_satoshi\n",
    "                            offset += 8\n",
    "                            script_len, offset = parse_varint(data, offset)\n",
    "                            offset += script_len\n",
    "                        \n",
    "                        offset += 4\n",
    "                        \n",
    "                        tx = {\n",
    "                            'timestamp': block_timestamp,\n",
    "                            'input_count': input_count,\n",
    "                            'output_count': output_count,\n",
    "                            'value_satoshi': total_value\n",
    "                        }\n",
    "                        transactions.append(tx)\n",
    "                        tx_count += 1\n",
    "                        \n",
    "                        if (tx_count % 100000) == 0:\n",
    "                            print(f\"  Extracted {tx_count:,} transactions...\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    break\n",
    "            else:\n",
    "                offset += 1\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  Error reading {dat_file_path.name}: {str(e)[:60]}\")\n",
    "    \n",
    "    return transactions\n",
    "\n",
    "print(\"\\nüìñ Step 1: Parsing .dat files...\")\n",
    "dat_folder = Path(\"data/blocks\")\n",
    "all_transactions = []\n",
    "\n",
    "if not dat_folder.exists():\n",
    "    print(f\"  WARNING: {dat_folder} not found!\")\n",
    "    print(f\"  Please ensure blockchain .dat files are in data/blocks/\")\n",
    "else:\n",
    "    dat_files = list(sorted(dat_folder.glob(\"blk*.dat\")))\n",
    "    print(f\"  Found {len(dat_files)} .dat files\")\n",
    "    \n",
    "    for idx, dat_file in enumerate(dat_files[:20], 1):\n",
    "        print(f\"\\n  [{idx}] Processing {dat_file.name}...\")\n",
    "        txs = parse_transactions_from_dat(dat_file)\n",
    "        all_transactions.extend(txs)\n",
    "        print(f\"      Extracted {len(txs):,} transactions (Total: {len(all_transactions):,})\")\n",
    "\n",
    "print(f\"\\n‚úÖ TOTAL: {len(all_transactions):,} real blockchain transactions extracted\")\n",
    "\n",
    "if len(all_transactions) > 0:\n",
    "    print(\"\\nüìä Step 2: Aggregating by hour...\")\n",
    "    df_txs = pd.DataFrame(all_transactions)\n",
    "    \n",
    "    df_txs['datetime'] = pd.to_datetime(df_txs['timestamp'], unit='s', utc=True)\n",
    "    df_txs['hour'] = df_txs['datetime'].dt.floor('h')\n",
    "    \n",
    "    df_hourly_real = df_txs.groupby('hour').agg({\n",
    "        'timestamp': 'count',\n",
    "        'input_count': ['mean', 'sum'],\n",
    "        'output_count': ['mean', 'sum'],\n",
    "        'value_satoshi': ['sum', 'mean', 'std']\n",
    "    }).reset_index()\n",
    "    \n",
    "    df_hourly_real.columns = [\n",
    "        'timestamp', 'tx_count', 'avg_inputs', 'total_inputs',\n",
    "        'avg_outputs', 'total_outputs', 'total_value_satoshi', \n",
    "        'avg_value_satoshi', 'std_value_satoshi'\n",
    "    ]\n",
    "    \n",
    "    for col in ['total_value_satoshi', 'avg_value_satoshi', 'std_value_satoshi']:\n",
    "        df_hourly_real[col] = df_hourly_real[col] / 100_000_000\n",
    "    \n",
    "    df_hourly_real = df_hourly_real.rename(columns={\n",
    "        'total_value_satoshi': 'total_volume_btc',\n",
    "        'avg_value_satoshi': 'avg_amount_btc',\n",
    "        'std_value_satoshi': 'std_amount_btc'\n",
    "    })\n",
    "    \n",
    "    print(f\"  Aggregated to {len(df_hourly_real):,} hourly records\")\n",
    "    print(f\"  Period: {df_hourly_real['timestamp'].min()} to {df_hourly_real['timestamp'].max()}\")\n",
    "    \n",
    "    print(f\"\\nüìã Statistics:\")\n",
    "    print(f\"  - tx_count: min={df_hourly_real['tx_count'].min():.0f}, max={df_hourly_real['tx_count'].max():.0f}, mean={df_hourly_real['tx_count'].mean():.0f}\")\n",
    "    print(f\"  - total_volume_btc: min={df_hourly_real['total_volume_btc'].min():.2f}, max={df_hourly_real['total_volume_btc'].max():.2f}\")\n",
    "    print(f\"  - avg_inputs: min={df_hourly_real['avg_inputs'].min():.1f}, max={df_hourly_real['avg_inputs'].max():.1f}\")\n",
    "    \n",
    "    print(f\"\\nüíæ Step 3: Saving to Parquet...\")\n",
    "    os.makedirs(\"data/parquet\", exist_ok=True)\n",
    "    blockchain_real_parquet = \"data/parquet/blockchain_real_hourly.parquet\"\n",
    "    df_hourly_real['timestamp'] = df_hourly_real['timestamp'].astype(str)\n",
    "    df_hourly_real.to_parquet(blockchain_real_parquet)\n",
    "    print(f\"  Saved: {blockchain_real_parquet}\")\n",
    "    print(f\"\\n‚úÖ BONUS PHASE COMPLETE - Real blockchain data ready!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: No transactions found. Check data/blocks/ folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b08b218",
   "metadata": {},
   "source": [
    "# PHASE 1: Blockchain + Prices Integration\n",
    "Uses real blockchain data extracted by BONUS PHASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52613caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PHASE 1: BLOCKCHAIN + PRICES INTEGRATION\n",
      "======================================================================\n",
      "\n",
      "üìñ Loading price data...\n",
      "‚úì 68,543 hours loaded\n",
      "\n",
      "üîó Checking for real blockchain data...\n",
      "‚úÖ REAL BLOCKCHAIN DATA FOUND - Using actual transactions!\n",
      "   ‚úì 1,888 hours of real data\n",
      "   ‚úì Period: 2012-06-09 22:00:00+00:00 to 2012-08-30 09:00:00+00:00\n",
      "\n",
      "üíæ Preparing blockchain data...\n",
      "‚úì Saved: data/parquet/blockchain_hourly.parquet\n",
      "\n",
      "‚ö° Loading into Spark...\n",
      "‚úì Prices: 68,543 rows\n",
      "‚úì Blockchain: 1,888 rows (REAL DATA)\n",
      "\n",
      "üîÑ Joining blockchain + prices...\n",
      "‚úì JOIN complete: 68,543 rows\n",
      "‚úÖ PHASE 1 COMPLETE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pyspark.sql.functions import col, to_timestamp, date_trunc\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 1: BLOCKCHAIN + PRICES INTEGRATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìñ Loading price data...\")\n",
    "df_prices_pd = pd.read_csv(\"data/prices/btc_1h_data_2018_to_2025.csv\")\n",
    "print(f\"‚úì {len(df_prices_pd):,} hours loaded\")\n",
    "\n",
    "print(\"\\nüîó Checking for real blockchain data...\")\n",
    "blockchain_real_parquet = \"data/parquet/blockchain_real_hourly.parquet\"\n",
    "\n",
    "if os.path.exists(blockchain_real_parquet):\n",
    "    print(\"‚úÖ REAL BLOCKCHAIN DATA FOUND - Using actual transactions!\")\n",
    "    df_hourly_blockchain = pd.read_parquet(blockchain_real_parquet)\n",
    "    df_hourly_blockchain['timestamp'] = pd.to_datetime(df_hourly_blockchain['timestamp'])\n",
    "    print(f\"   ‚úì {len(df_hourly_blockchain):,} hours of real data\")\n",
    "    print(f\"   ‚úì Period: {df_hourly_blockchain['timestamp'].min()} to {df_hourly_blockchain['timestamp'].max()}\")\n",
    "    is_real_blockchain = True\n",
    "else:\n",
    "    print(\"‚ùå ERROR: Real blockchain data not found!\")\n",
    "    print(\"   Please run BONUS PHASE cell first to extract blockchain data.\")\n",
    "    raise FileNotFoundError(f\"Missing: {blockchain_real_parquet}\")\n",
    "\n",
    "print(\"\\nüíæ Preparing blockchain data...\")\n",
    "df_hourly_blockchain['timestamp'] = df_hourly_blockchain['timestamp'].astype(str)\n",
    "required_cols = ['timestamp', 'tx_count', 'avg_inputs', 'avg_outputs', 'total_volume_btc', 'avg_amount_btc', 'std_amount_btc']\n",
    "df_hourly_blockchain = df_hourly_blockchain[required_cols]\n",
    "\n",
    "os.makedirs(\"data/parquet\", exist_ok=True)\n",
    "blockchain_parquet = \"data/parquet/blockchain_hourly.parquet\"\n",
    "df_hourly_blockchain.to_parquet(blockchain_parquet)\n",
    "print(f\"‚úì Saved: {blockchain_parquet}\")\n",
    "\n",
    "print(\"\\n‚ö° Loading into Spark...\")\n",
    "df_prices_spark = spark.read.csv(\"data/prices/btc_1h_data_2018_to_2025.csv\", header=True, inferSchema=True)\n",
    "print(f\"‚úì Prices: {df_prices_spark.count():,} rows\")\n",
    "\n",
    "df_blockchain_spark = spark.read.parquet(blockchain_parquet)\n",
    "print(f\"‚úì Blockchain: {df_blockchain_spark.count():,} rows (REAL DATA)\")\n",
    "\n",
    "print(\"\\nüîÑ Joining blockchain + prices...\")\n",
    "df_prices_spark = df_prices_spark.withColumn(\n",
    "    \"hour_ts\", \n",
    "    date_trunc('hour', to_timestamp(col(\"Open time\")))\n",
    ")\n",
    "\n",
    "df_blockchain_spark = df_blockchain_spark.withColumn(\n",
    "    \"hour_ts\", \n",
    "    to_timestamp(col(\"timestamp\"))\n",
    ")\n",
    "\n",
    "df_prices_spark.createOrReplaceTempView(\"prices\")\n",
    "df_blockchain_spark.createOrReplaceTempView(\"blockchain\")\n",
    "\n",
    "df_integrated = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        p.*,\n",
    "        b.tx_count,\n",
    "        b.avg_inputs,\n",
    "        b.avg_outputs,\n",
    "        b.total_volume_btc,\n",
    "        b.avg_amount_btc,\n",
    "        b.std_amount_btc\n",
    "    FROM prices p\n",
    "    LEFT JOIN blockchain b\n",
    "    ON p.hour_ts = b.hour_ts\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úì JOIN complete: {df_integrated.count():,} rows\")\n",
    "df_integrated.cache()\n",
    "print(f\"‚úÖ PHASE 1 COMPLETE\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecab114",
   "metadata": {},
   "source": [
    "# PHASE 2: Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a075cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHASE 2: DATA CLEANING\n",
      "======================================================================\n",
      "‚úì 68,542 rows cleaned\n",
      "‚úÖ PHASE 2 COMPLETE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lead, when, col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "print(\"PHASE 2: DATA CLEANING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df_clean = df_integrated.filter(col(\"Open time\").isNotNull()).sort(\"Open time\")\n",
    "\n",
    "window_spec = Window.orderBy(\"Open time\")\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"next_close\",\n",
    "    lead(\"Close\").over(window_spec)\n",
    ")\n",
    "\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"price_direction\",\n",
    "    when(col(\"next_close\") > col(\"Close\"), 1).otherwise(0)\n",
    ").filter(col(\"price_direction\").isNotNull())\n",
    "\n",
    "print(f\"‚úì {df_clean.count():,} rows cleaned\")\n",
    "print(f\"‚úÖ PHASE 2 COMPLETE\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5209efdd",
   "metadata": {},
   "source": [
    "# PHASE 3: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd415196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHASE 3: FEATURE ENGINEERING\n",
      "======================================================================\n",
      "\n",
      "Creating 14 features (6 price + 8 blockchain)...\n",
      "‚úì 68,542 rows with 14 features\n",
      "‚úÖ PHASE 3 COMPLETE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, lag, avg, stddev, log, \n",
    "    abs as spark_abs, round as spark_round\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "print(\"PHASE 3: FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "window_24h = Window.orderBy(\"Open time\").rowsBetween(-24, 0)\n",
    "window_7d = Window.orderBy(\"Open time\").rowsBetween(-168, 0)\n",
    "\n",
    "df_features = df_clean.filter(col(\"Open time\").isNotNull())\n",
    "\n",
    "print(\"\\nCreating 14 features (6 price + 8 blockchain)...\")\n",
    "df_features = df_features.withColumn(\n",
    "    \"hourly_return\",\n",
    "    spark_round(((col(\"Close\") - col(\"Open\")) / col(\"Open\")) * 100, 4)\n",
    ").withColumn(\n",
    "    \"hl_range\",\n",
    "    spark_round((col(\"High\") - col(\"Low\")) / col(\"Open\") * 100, 4)\n",
    ").withColumn(\n",
    "    \"close_ma_24h\",\n",
    "    spark_round(avg(\"Close\").over(window_24h), 2)\n",
    ").withColumn(\n",
    "    \"close_ma_7d\",\n",
    "    spark_round(avg(\"Close\").over(window_7d), 2)\n",
    ").withColumn(\n",
    "    \"volatility_24h\",\n",
    "    spark_round(stddev(\"Close\").over(window_24h), 2)\n",
    ").withColumn(\n",
    "    \"volume_ratio\",\n",
    "    spark_round(col(\"Volume\") / avg(\"Volume\").over(window_24h), 4)\n",
    ").withColumn(\n",
    "    \"tx_count_ratio\",\n",
    "    spark_round(col(\"tx_count\") / avg(\"tx_count\").over(window_24h), 4)\n",
    ").withColumn(\n",
    "    \"volume_btc_ratio\",\n",
    "    spark_round(col(\"total_volume_btc\") / avg(\"total_volume_btc\").over(window_24h), 4)\n",
    ").withColumn(\n",
    "    \"avg_inputs_log\",\n",
    "    spark_round(log(col(\"avg_inputs\") + 1), 4)\n",
    ").withColumn(\n",
    "    \"avg_outputs_log\",\n",
    "    spark_round(log(col(\"avg_outputs\") + 1), 4)\n",
    ").withColumn(\n",
    "    \"price_to_onchain\",\n",
    "    spark_round(col(\"Close\") / (col(\"total_volume_btc\") + 1), 4)\n",
    ").withColumn(\n",
    "    \"io_ratio\",\n",
    "    spark_round(col(\"avg_inputs\") / (col(\"avg_outputs\") + 0.1), 4)\n",
    ")\n",
    "\n",
    "print(f\"‚úì {df_features.count():,} rows with 14 features\")\n",
    "print(f\"‚úÖ PHASE 3 COMPLETE\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456ff756",
   "metadata": {},
   "source": [
    "# PHASE 4: ML Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3514541c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHASE 4: ML DATA PREPARATION\n",
      "======================================================================\n",
      "\n",
      "Preparing 14 features...\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o228.fit.\n: org.apache.spark.SparkException: surrogate cannot be computed. All the values in tx_count,tx_count_ratio,total_volume_btc,volume_btc_ratio,avg_inputs_log,avg_outputs_log,price_to_onchain,io_ratio are Null, Nan or missingValue(NaN)\r\n\tat org.apache.spark.ml.feature.Imputer.fit(Imputer.scala:204)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPreparing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(feature_cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m imputer \u001b[38;5;241m=\u001b[39m Imputer(inputCols\u001b[38;5;241m=\u001b[39mfeature_cols, outputCols\u001b[38;5;241m=\u001b[39mfeature_cols, strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m df_ml \u001b[38;5;241m=\u001b[39m \u001b[43mimputer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_features\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(df_features)\n\u001b[0;32m     17\u001b[0m df_ml \u001b[38;5;241m=\u001b[39m df_ml\u001b[38;5;241m.\u001b[39mfilter(col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice_direction\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39misNotNull())\n\u001b[0;32m     19\u001b[0m assembler \u001b[38;5;241m=\u001b[39m VectorAssembler(inputCols\u001b[38;5;241m=\u001b[39mfeature_cols, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures_raw\u001b[39m\u001b[38;5;124m'\u001b[39m, handleInvalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\bda-env\\lib\\site-packages\\pyspark\\ml\\base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    208\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\bda-env\\lib\\site-packages\\pyspark\\ml\\util.py:164\u001b[0m, in \u001b[0;36mtry_remote_fit.<locals>.wrapped\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\bda-env\\lib\\site-packages\\pyspark\\ml\\wrapper.py:411\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;129m@try_remote_fit\u001b[39m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 411\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    412\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\bda-env\\lib\\site-packages\\pyspark\\ml\\wrapper.py:407\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\bda-env\\lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\bda-env\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\bda-env\\lib\\site-packages\\py4j\\protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o228.fit.\n: org.apache.spark.SparkException: surrogate cannot be computed. All the values in tx_count,tx_count_ratio,total_volume_btc,volume_btc_ratio,avg_inputs_log,avg_outputs_log,price_to_onchain,io_ratio are Null, Nan or missingValue(NaN)\r\n\tat org.apache.spark.ml.feature.Imputer.fit(Imputer.scala:204)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.sql.functions import col, year, when, lit, coalesce, avg\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "print(\"PHASE 4: ML DATA PREPARATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# First, fill NULL blockchain columns with 0 (no activity)\n",
    "print(\"\\nüîß Step 1: Handling NULL blockchain columns...\")\n",
    "df_filled = df_features.withColumn(\n",
    "    \"tx_count\", coalesce(col(\"tx_count\"), lit(0))\n",
    ").withColumn(\n",
    "    \"avg_inputs\", coalesce(col(\"avg_inputs\"), lit(1.0))\n",
    ").withColumn(\n",
    "    \"avg_outputs\", coalesce(col(\"avg_outputs\"), lit(1.0))\n",
    ").withColumn(\n",
    "    \"total_volume_btc\", coalesce(col(\"total_volume_btc\"), lit(0.0))\n",
    ").withColumn(\n",
    "    \"avg_amount_btc\", coalesce(col(\"avg_amount_btc\"), lit(0.0))\n",
    ").withColumn(\n",
    "    \"std_amount_btc\", coalesce(col(\"std_amount_btc\"), lit(0.0))\n",
    ")\n",
    "\n",
    "print(f\"‚úì Filled NULL values (replaced with defaults)\")\n",
    "\n",
    "feature_cols = [\n",
    "    'hourly_return', 'hl_range', 'close_ma_24h', 'close_ma_7d', 'volatility_24h', 'volume_ratio',\n",
    "    'tx_count', 'tx_count_ratio', 'total_volume_btc', 'volume_btc_ratio',\n",
    "    'avg_inputs_log', 'avg_outputs_log', 'price_to_onchain', 'io_ratio'\n",
    "]\n",
    "\n",
    "print(f\"\\nüìä Step 2: Preparing {len(feature_cols)} features...\")\n",
    "\n",
    "# Check for any remaining NaN/NULL\n",
    "print(\"  Checking for NaN values...\")\n",
    "for feat in feature_cols:\n",
    "    null_count = df_filled.filter(col(feat).isNull()).count()\n",
    "    nan_count = df_filled.filter((col(feat) != col(feat)) | col(feat).isNaN()).count()\n",
    "    if null_count > 0 or nan_count > 0:\n",
    "        print(f\"    WARNING: {feat} has {null_count} NULLs + {nan_count} NaNs\")\n",
    "\n",
    "# Replace any remaining NaN with 0\n",
    "print(\"  Replacing remaining NaN/NULL with 0...\")\n",
    "for feat in feature_cols:\n",
    "    df_filled = df_filled.withColumn(\n",
    "        feat, \n",
    "        when(col(feat).isNaN() | col(feat).isNull(), 0.0).otherwise(col(feat))\n",
    "    )\n",
    "\n",
    "df_filled = df_filled.filter(col('price_direction').isNotNull())\n",
    "print(f\"‚úì {df_filled.count():,} rows with valid price_direction\")\n",
    "\n",
    "# Assemble features\n",
    "print(\"\\nüîó Step 3: Assembling feature vector...\")\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features_raw', handleInvalid='skip')\n",
    "df_ml = assembler.transform(df_filled)\n",
    "print(f\"‚úì Features assembled\")\n",
    "\n",
    "# Scale features\n",
    "print(\"\\n‚öñÔ∏è  Step 4: Scaling features...\")\n",
    "scaler = StandardScaler(inputCol='features_raw', outputCol='features', withMean=True, withStd=True)\n",
    "scaler_model = scaler.fit(df_ml)\n",
    "df_ml = scaler_model.transform(df_ml)\n",
    "print(f\"‚úì Features scaled with StandardScaler\")\n",
    "\n",
    "# Split by year\n",
    "print(\"\\nüìÖ Step 5: Splitting train/test by year...\")\n",
    "df_ml = df_ml.withColumn('year', year('Open time'))\n",
    "df_train = df_ml.filter(col('year') < 2024)\n",
    "df_test = df_ml.filter(col('year') >= 2024)\n",
    "\n",
    "train_count = df_train.count()\n",
    "test_count = df_test.count()\n",
    "\n",
    "print(f\"\\n‚úÖ Data split complete:\")\n",
    "print(f\"  Train: {train_count:,} rows (2018-2023)\")\n",
    "print(f\"  Test: {test_count:,} rows (2024-2025)\")\n",
    "\n",
    "if train_count == 0 or test_count == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  ERROR: Empty train or test set!\")\n",
    "    raise ValueError(\"Train or test set is empty!\")\n",
    "\n",
    "print(f\"\\n‚úÖ PHASE 4 COMPLETE\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d1fc19",
   "metadata": {},
   "source": [
    "# PHASE 5: Model Training with Real Blockchain Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce3e7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "print(\"PHASE 5: MODEL TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nTraining Logistic Regression...\")\n",
    "lr = LogisticRegression(labelCol='price_direction', featuresCol='features', maxIter=100, regParam=0.01)\n",
    "model_lr = lr.fit(df_train)\n",
    "pred_lr = model_lr.transform(df_test)\n",
    "\n",
    "print(\"Training Random Forest (100 trees, depth 15)...\")\n",
    "rf = RandomForestClassifier(labelCol='price_direction', featuresCol='features', numTrees=100, maxDepth=15, seed=42)\n",
    "model_rf = rf.fit(df_train)\n",
    "pred_rf = model_rf.transform(df_test)\n",
    "\n",
    "evaluator_auc = BinaryClassificationEvaluator(labelCol='price_direction', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
    "evaluator_acc = MulticlassClassificationEvaluator(labelCol='price_direction', predictionCol='prediction', metricName='accuracy')\n",
    "\n",
    "auc_lr = evaluator_auc.evaluate(pred_lr)\n",
    "acc_lr = evaluator_acc.evaluate(pred_lr)\n",
    "auc_rf = evaluator_auc.evaluate(pred_rf)\n",
    "acc_rf = evaluator_acc.evaluate(pred_rf)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS (with Real Blockchain Data)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'Model':<20} {'AUC':<15} {'Accuracy':<15}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Logistic Reg':<20} {auc_lr:<15.4f} {acc_lr:<15.4f}\")\n",
    "print(f\"{'Random Forest':<20} {auc_rf:<15.4f} {acc_rf:<15.4f}\")\n",
    "\n",
    "best_auc = max(auc_lr, auc_rf)\n",
    "best_model = \"Random Forest\" if auc_rf == best_auc else \"Logistic Regression\"\n",
    "improvement = ((best_auc - 0.5) / 0.5) * 100\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model}\")\n",
    "print(f\"   AUC Score: {best_auc:.4f}\")\n",
    "print(f\"   Improvement vs random (0.5): {improvement:+.1f}%\")\n",
    "print(f\"\\n‚úÖ TRAINING COMPLETE!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
